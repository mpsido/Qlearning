{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from random import randrange\n",
    "\n",
    "def random_argmax(vector):\n",
    "    \"\"\" Argmax that chooses randomly among eligible maximum indices. \"\"\"\n",
    "    m = np.amax(vector)\n",
    "    indices = np.nonzero(vector == m)[0]\n",
    "    if len(indices) == 0:\n",
    "        raise IndexError(vector, m, indices)\n",
    "    return np.random.choice(indices)\n",
    "\n",
    "class CarRental():    \n",
    "    def reset(self):\n",
    "        self.nb_location = 2\n",
    "        self.car_request = (3, 4) \n",
    "        self.car_return = (3, 2)\n",
    "        self.state = [0, 0]\n",
    "        self.rent_reward = 10\n",
    "        self.transfer_cost = 2\n",
    "        self.reward = 0\n",
    "        self.location_capacity = 20\n",
    "        return self.state, self.reward, False, {}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        class ActionSpace:\n",
    "            def sample(self):\n",
    "                return randrange(self.n+1)-self.location_capacity\n",
    "        self.action_space = ActionSpace()\n",
    "        self.action_space.n = self.location_capacity*2+1\n",
    "        \n",
    "    # For with statement\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        self.close()\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def rent_and_return(self, log = False):\n",
    "        for i in range(self.nb_location):\n",
    "            nb_rent_request = np.random.poisson(self.car_request[i], 1)[0]\n",
    "            nb_return = np.random.poisson(self.car_return[i], 1)[0]\n",
    "            if log: print(\"Rent, Return:\", nb_rent_request, nb_return)\n",
    "            # rent out cars\n",
    "            nb_rent = nb_rent_request\n",
    "            if (nb_rent_request > self.state[i]):\n",
    "                if log: print(\"!!! Refusing\", nb_rent_request - self.state[i], \"rentals on location\", i)\n",
    "                nb_rent = self.state[i]\n",
    "            if log: print(\"Renting\", nb_rent, \"from location \", i, \"nb cars:\", self.state[i])\n",
    "            self.state[i] -= nb_rent\n",
    "            self.reward += self.rent_reward * nb_rent\n",
    "            if log: print(\"State:\", self.state)\n",
    "            if log: print(\"Reward:\", self.reward)\n",
    "            # return cars\n",
    "            self.state[i] = min(self.location_capacity, self.state[i] + nb_return)\n",
    "            if log: print(\"Return\", nb_return, \"on location\", i, \"nb cars:\", self.state[i])\n",
    "\n",
    "    def step(self, action):\n",
    "        self.perform_action(action)\n",
    "        return tuple(self.state), self.reward, False, {}\n",
    "    \n",
    "    def perform_action(self, action):\n",
    "        state, cost = self.get_action_output(self.state, action)\n",
    "        self.state = list(state)\n",
    "        self.reward -= cost\n",
    "        \n",
    "    def get_action_output(self, state, action):\n",
    "        out_state = [0, 0]\n",
    "        if (action > 0):\n",
    "            effective_action = min(action, state[0])\n",
    "        elif (action < 0):\n",
    "            effective_action = -min(-action, state[1])\n",
    "        else:\n",
    "            return tuple(state), 0\n",
    "        out_state[0] = min(state[0] - effective_action, self.location_capacity)\n",
    "        out_state[1] = min(state[1] + effective_action, self.location_capacity)\n",
    "        cost = self.transfer_cost * abs(action)\n",
    "        if (action >= 1):\n",
    "            cost -= 2\n",
    "        if abs(effective_action) != abs(action):\n",
    "            cost += 10*abs(abs(action)-abs(effective_action))\n",
    "        return tuple(out_state), cost\n",
    "    \n",
    "    def expected_reward(self, state):\n",
    "        shop1ProbRewardTuples = []\n",
    "        p_r_1 = 0.0\n",
    "        for i in range(state[0]+1):\n",
    "            r = self.rent_reward * i\n",
    "            l_ambda = self.car_request[0]\n",
    "            p_r = math.pow(l_ambda, i) * math.exp(-l_ambda) / math.factorial(i)\n",
    "            p_r_1 += p_r\n",
    "            shop1ProbRewardTuples.append((r, p_r))\n",
    "        shop1ProbRewardTuples.append((self.rent_reward*state[0], 1-p_r_1))\n",
    "        \n",
    "        shop2ProbRewardTuples = []\n",
    "        p_r_2 = 0.0\n",
    "        for i in range(state[1]+1):\n",
    "            r = self.rent_reward * i\n",
    "            l_ambda = self.car_request[1]\n",
    "            p_r = math.pow(l_ambda, i) * math.exp(-l_ambda) / math.factorial(i)\n",
    "            p_r_2 += p_r\n",
    "            shop2ProbRewardTuples.append((r, p_r))\n",
    "        shop2ProbRewardTuples.append((self.rent_reward*state[1], 1-p_r_2))\n",
    "        \n",
    "        probs = []\n",
    "        probRewardTuples = []\n",
    "        for j, k in np.ndindex((len(shop1ProbRewardTuples), len(shop2ProbRewardTuples))):\n",
    "            p1, p2 = shop1ProbRewardTuples[j][1], shop2ProbRewardTuples[k][1]\n",
    "            probs.append(p1*p2)\n",
    "            r1, r2 = shop1ProbRewardTuples[j][0], shop2ProbRewardTuples[k][0]\n",
    "            probRewardTuples.append((r1+r2, p1*p2))\n",
    "        \n",
    "        sum_probs = sum(probs)\n",
    "        if abs(sum_probs-1) > 0.0001:\n",
    "            raise ValueError(\"Probs should sum up to 1\", sum_probs, probs)\n",
    "        return probRewardTuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment specific functions\n",
    "\n",
    "def states():\n",
    "    for s1, s2 in np.ndindex((21, 21)):\n",
    "        yield (s1, s2)\n",
    "        \n",
    "def init_Vs(V_s):\n",
    "    for state in states():\n",
    "        if state not in V_s:\n",
    "            V_s[state] = 0.0\n",
    "    \n",
    "def p_next(env, s, a):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples (s', r, P(s', r|s, a), a)\n",
    "    Containing the probability P(s', r) of reaching state s' with reward r\n",
    "    when taking action a from state s\n",
    "    \"\"\"\n",
    "    next_state, cost = env.get_action_output(s, a)\n",
    "    probRewardList = env.expected_reward(next_state)\n",
    "    return [ (next_state,)+(pr[0]-cost, pr[1], a) for pr in probRewardList ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CarRental()\n",
    "V_s = {}\n",
    "gamma = 0.9\n",
    "state, reward, done, info = env.reset()\n",
    "\n",
    "def value_of_state(env, V_s, state):\n",
    "    actions = exhaustive_greedy_policy(env, V_s, state)\n",
    "    v = 0.0\n",
    "    for action in actions:\n",
    "        next_state, _ = env.get_action_output(state, action)\n",
    "        rewardsDistribution = p_next(env, next_state, action)\n",
    "        for (next_state, reward, prob, a) in rewardsDistribution:\n",
    "            if a == action:\n",
    "                v += prob*(reward+gamma*V_s[next_state])/len(actions)\n",
    "    return v\n",
    "\n",
    "def epsilon_greedy_policy(env, V_s, state, epsilon):\n",
    "    if np.random.rand(1) < epsilon:\n",
    "        action = self.env.action_space.sample()\n",
    "        return action\n",
    "    return greedy_policy(env, V_s, state)\n",
    "\n",
    "def Q(env, V_s, state):\n",
    "    values = []\n",
    "    for action in range(-20, 21):\n",
    "        next_state, _ = env.get_action_output(state, action)\n",
    "        rewardsDistribution = p_next(env, next_state, action)\n",
    "        v = 0.0\n",
    "        for (next_state, reward, prob, a) in rewardsDistribution:\n",
    "            if a == action:\n",
    "                v += prob*(reward+gamma*V_s[next_state])\n",
    "        values.append(v)\n",
    "        #values.append(V_s[next_state])\n",
    "    if len(values) == 0:\n",
    "        raise RangeError(values)\n",
    "    return values\n",
    "\n",
    "def exhaustive_greedy_policy(env, V_s, state):\n",
    "    values = Q(env, V_s, state)\n",
    "    return [i-20 for i, v in enumerate(values) if v==max(values)]\n",
    "    \n",
    "def greedy_policy(env, V_s, state):\n",
    "    values = Q(env, V_s, state)\n",
    "    return random_argmax(values)-20\n",
    "\n",
    "def fixed_greedy_policy(env, V_s, state):\n",
    "    values = Q(env, V_s, state)\n",
    "    return np.argmax(values)-20\n",
    "\n",
    "def update_V_s(env, V_s):\n",
    "    fixedV_s = V_s.copy()\n",
    "    for state in states():\n",
    "        v = value_of_state(env, fixedV_s, state)\n",
    "        if math.isnan(v) == False:\n",
    "            V_s[state] = v\n",
    "\n",
    "def policy_udate(env, V_s):\n",
    "    pi = {}\n",
    "    pi_previous = {}\n",
    "    hasChange = True\n",
    "    while hasChange:\n",
    "        for state in states():\n",
    "            pi[state] = greedy_policy(env, V_s, state)\n",
    "        update_V_s(env, V_s)\n",
    "        hasChange = False\n",
    "        for v in pi:\n",
    "            if (v not in pi_previous) or (pi[v] != pi_previous[v]):\n",
    "                hasChange = True\n",
    "                if v in pi_previous:\n",
    "                    print(\"In state {} changing action {} for {}\".format(v, pi_previous[v], pi[v]))\n",
    "                break\n",
    "        pi_previous = pi.copy()\n",
    "\n",
    "init_Vs(V_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_udate(env, V_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "for state in states():\n",
    "    choice = exhaustive_greedy_policy(env, V_s, state)\n",
    "    print(state, value_of_state(env, V_s, state), choice)\n",
    "    if len(choice) != 1:\n",
    "        print(\"Many choices for state\", state)\n",
    "    policy[state] = choice[0]\n",
    "    \n",
    "def memorized_greedy(env, V_s, state):\n",
    "    return policy[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing_policy(*_):\n",
    "    return 0\n",
    "\n",
    "def random_policy(_, V_s, state):\n",
    "    if state[0] == state[1]:\n",
    "        return 0\n",
    "    return randrange(-state[0], state[1])\n",
    "\n",
    "def test_policy(env, nb_episodes, nb_steps, policy):\n",
    "    rewards = []\n",
    "    for i in range(nb_episodes):\n",
    "        env.reset()\n",
    "        for j in range(nb_steps):\n",
    "            env.rent_and_return()\n",
    "            action = policy(env, V_s, state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "    return sum(rewards)/len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average reward do nothing policy:\", test_policy(env, 200, 100, do_nothing_policy))\n",
    "print(\"Average reward random policy:    \", test_policy(env, 200, 100, random_policy))\n",
    "print(\"Average reward optimal policy:   \", test_policy(env, 200, 100, memorized_greedy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
