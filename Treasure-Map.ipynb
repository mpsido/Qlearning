{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treasure_cave\n",
    "import importlib\n",
    "importlib.reload(treasure_cave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game=treasure_cave.TreasureCave()\n",
    "game.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info=game.play_step('a')\n",
    "if done and reward > 0:\n",
    "    print(\"You Win!\")\n",
    "elif done and reward < 0:\n",
    "    print(\"You Loose!\")\n",
    "game.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the machine to find the treasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=treasure_cave.TreasureCave()\n",
    "def state_function(state_function):\n",
    "    return state[0], state[1][0], state[1][1]\n",
    "# env.action_space.n=4\n",
    "\n",
    "game = qlearning.GamePlayer(env, state_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "decay_rate = 0.01\n",
    "epsilon = 0.9\n",
    "#game.erase_training()\n",
    "rewards = game.train(total_episodes, alpha, gamma, epsilon, decay_rate, logEvery=100)\n",
    "print(\"Total reward average:\", np.mean(rewards))\n",
    "print(len(game.qtable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_function = game.q_trained_action\n",
    "qlearning.visualize_computer_playing(15, env, action_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hack the reward system to influence the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine rewards\n",
    "import types\n",
    "\n",
    "env.step_backup = env.step\n",
    "    \n",
    "def step(self, action):\n",
    "    state, reward, done, info = self.step_backup(action)\n",
    "    if reward == 0:\n",
    "        reward = -1\n",
    "    elif reward == -1:\n",
    "        reward = -20\n",
    "    elif reward == 1:\n",
    "        reward = 20\n",
    "    return state, reward, done, info\n",
    "\n",
    "env.step = types.MethodType(step, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "decay_rate = 0\n",
    "epsilon = 0.1\n",
    "#game.erase_training()\n",
    "rewards = game.train(total_episodes, alpha, gamma, epsilon, decay_rate, logEvery=100)\n",
    "print(\"Total reward average:\", np.mean(rewards))\n",
    "print(len(game.qtable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_function = game.q_trained_action\n",
    "qlearning.visualize_computer_playing(15, env, action_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement Learning",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
